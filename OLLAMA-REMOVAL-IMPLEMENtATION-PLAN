# Ollama Removal Implementation Plan

Goal: Remove the Ollama backend and rely solely on llama.cpp while preserving the provider pattern so other backends can be added later. All current modes must work with llama.cpp except ragMode, which will be disabled or removed. All commands must work with llama.cpp, including `list models`.

Non-goals (for this change):
- Removing the provider abstraction or refactoring to a single hard-coded provider.
- Adding a new provider beyond llama.cpp.
- Rewriting CLI UX unrelated to provider selection.

Confirmed Decisions
1) The default provider is llama.cpp whenever a provider is required.
2) All config examples must only show llama.cpp hosts.
3) All existing modes must function with llama.cpp, except ragMode which will be disabled or removed.
4) All commands must work with llama.cpp; Ollama-only behavior must be reimplemented for llama.cpp.
5) Benchmark server behavior should drop Ollama model-resolution logic and only support llama.cpp inputs.

Inventory of Ollama Touchpoints (from code search)
- CLI: `cli/cli.go`, `cli/cli_pipeline.go`, `internal/cli/root.go`
- Provider factory: `internal/providerfactory/factory.go`
- Providers: `internal/providers/ollama/*`, `internal/providers/mcp/*`, `internal/providers/multiplex/*`
- Models: `internal/models/*` (OllamaHost, metadata, commands)
- Config examples: `config/*.json` and `config/config.example.*.json`
- Docs: `README.md`, `RAG_IMPLEMENTATION.md`
- Benchmarks server: `servers/benchmark/*`
- Tests: `internal/providers/ollama/*_test.go`, `internal/models/*_test.go`, CLI tests

Phase 1: Discovery and Baseline
1) Build a compatibility matrix for llama.cpp-only operation:
   - list models (must work)
   - unload models (must work)
   - JSON mode (must work)
   - MCP mode / tool calls (must work)
   - pull/delete/sync models (implement for llama.cpp)
   - list modelparameters (implement for llama.cpp)
2) Identify external config/docs referencing Ollama endpoints and Ollama-only CLI features.
3) Identify dependencies on Ollama request schemas (e.g., /api/chat, /api/generate, /api/ps) that need llama.cpp equivalents.

Phase 2: Provider Layer Adjustments
1) Remove the Ollama provider package:
   - Delete `internal/providers/ollama/*` and its tests.
   - Remove Ollama-specific tool formatting code (e.g., `tools.go`).
2) Update provider interfaces and factories:
   - In `internal/providers/provider.go`, update comments to remove Ollama references.
   - In `internal/providerfactory/factory.go`, remove Ollama imports and Ollama selection paths.
   - Ensure default provider selection points to llama.cpp (MCP remains but falls back to llama.cpp).
3) Update multiplex provider:
   - Remove any hard-coded reference to `"ollama"` as default.
   - Ensure routing logic only recognizes llama.cpp (and MCP if still used).
4) Update MCP provider fallback:
   - MCP remains; switch fallback provider to llama.cpp.
   - Implement llama.cpp-compatible tool-call forwarding (or emulate at the client if required).
   - Remove log messages referencing Ollama fallback.

Phase 3: Models and Host Management
1) Remove OllamaHost implementation:
   - Delete `internal/models/ollama_host.go` and any references.
2) Add/extend llama.cpp host capabilities to cover all required commands:
   - Implement list models against llama.cpp OpenAI-compatible endpoints.
   - Implement unload logic for llama.cpp if supported (or emulate via model management endpoints/router mode).
   - Implement model parameters and model inventory behaviors for llama.cpp equivalents.
   - Implement pull/delete/sync behaviors against llama.cpp model management endpoints or local model registry logic as needed.
3) Update model metadata:
   - In `internal/models/metadata.go`, remove Ollama-specific discovery and metadata fetch.
   - Ensure metadata supports llama.cpp only (OpenAI-compatible models).
4) Update model commands:
   - `internal/models/commands.go` should be llama.cpp-only.
   - Ensure list/unload/pull/delete/sync/modelparameters map to llama.cpp equivalents.
5) Update types and validation:
   - In `internal/models/types.go` and config validation, only accept `"llama.cpp"` (and `"llamacpp"` if alias retained).
   - Ensure error messages and docs mention only llama.cpp.

Phase 4: CLI and UX
1) Remove CLI references to Ollama provider:
   - `cli/cli.go` and `cli/cli_pipeline.go`: delete Ollama imports and fallback logic.
   - Replace any fallback log messages referencing Ollama.
2) Rework Ollama-only commands to operate on llama.cpp:
   - `agon list models` must query llama.cpp.
   - `agon pull models`, `agon delete models`, `agon sync models`, `agon list modelparameters` must be implemented for llama.cpp.
3) Update help text and command descriptions:
   - `internal/cli/root.go` Short description should mention llama.cpp only (or "OpenAI-compatible llama.cpp API").
4) Ensure provider selection logic in CLI always uses llama.cpp hosts when present.

Phase 5: Config Examples and Defaults
1) Replace all Ollama config examples:
   - All `config/*.json` and `config/config.example.*.json` should use llama.cpp host entries.
   - Replace `type: "ollama"` with `type: "llama.cpp"` (or `"llamacpp"` if alias retained).
   - Update host names from `Ollama01` -> `LlamaCpp01` (or similar).
2) Update `config.example.LlamaCpp.json` to be canonical and consistent with other examples.
3) Remove any reference to Ollama endpoints (URLs, placeholders).

Phase 6: Documentation Updates
1) Update `README.md`:
   - Replace Ollama-centric language with llama.cpp-only statements.
   - Update sections for pull/delete/sync models and model parameters to llama.cpp behavior.
   - Update feature list to reflect llama.cpp-only behavior.
   - Update config examples in the README to llama.cpp.
2) Update `RAG_IMPLEMENTATION.md`:
   - Remove references to Ollama embedding models and tokenization calls.
   - If ragMode is removed/disabled, replace with a deprecation note or remove the doc.
3) Update any screenshot references or labels (e.g., `agon_gollama.png`) if appropriate.

Phase 7: Benchmark Server Changes
1) Update `servers/benchmark/benchmark.yml` to only support llama.cpp.
2) Remove Ollama model resolution paths in `servers/benchmark/main.go`:
   - Remove manifest resolution, models path logic, and error messages.
   - Ensure request validation only accepts llama.cpp.
3) Update benchmarks documentation (if any) to reflect llama.cpp inputs.

Phase 8: Tests and Cleanup
1) Remove Ollama-specific tests:
   - `internal/providers/ollama/*_test.go`
   - `internal/models/models_test.go` sections referencing OllamaHost
   - CLI tests referencing `type: "ollama"`
2) Update remaining tests to use llama.cpp-only config fixtures.
3) Run search for "ollama" and ensure only historical references remain where explicitly desired (if any).

Phase 9: Validation Checklist
1) Build and run core CLI flows:
   - `agon list hosts`
   - `agon list models`
   - `agon chat` (single and multi-host modes)
   - JSON mode (ensure llama.cpp response format path works)
2) Confirm MCP mode behavior:
   - Verify tool call path works with llama.cpp.
3) Ensure config parsing accepts llama.cpp only.
4) Verify benchmarks server starts and handles llama.cpp models.
5) Run `rg -n "ollama"` to confirm removal.

Phase 10: Migration Notes (Optional)
1) Provide a short "breaking changes" note:
   - Ollama host types removed.
   - Ollama-only commands removed/converted to llama.cpp equivalents.
   - Config conversion instructions.
2) Provide a simple conversion example:
   - Old: { name: "Ollama01", type: "ollama", url: "http://..." }
   - New: { name: "LlamaCpp01", type: "llama.cpp", url: "http://..." }

Ordered Checklist
1) Lock scope: confirm ragMode removal/disablement path and llama.cpp equivalents for pull/delete/sync/modelparameters.
2) Map llama.cpp endpoints/capabilities needed for list/unload/model management; document gaps.
3) Remove `internal/providers/ollama/*` and update provider interfaces/comments.
4) Update `internal/providerfactory/factory.go` and `internal/providers/multiplex/*` for llama.cpp-only routing.
5) Update MCP provider fallback to llama.cpp and implement tool-call forwarding for llama.cpp.
6) Remove `internal/models/ollama_host.go`; add/extend llama.cpp host model-management methods.
7) Update `internal/models/metadata.go` to llama.cpp-only discovery and metadata.
8) Update `internal/models/commands.go` to use llama.cpp for list/unload/pull/delete/sync/modelparameters.
9) Update CLI wiring in `cli/cli.go` and `cli/cli_pipeline.go` to use llama.cpp provider only.
10) Update `internal/cli/root.go` help text and any CLI descriptions referencing Ollama.
11) Update config examples in `config/*.json` and `config/config.example.*.json` to llama.cpp-only.
12) Update README config examples, feature list, and command descriptions for llama.cpp.
13) Deprecate/remove ragMode docs in `RAG_IMPLEMENTATION.md`.
14) Update benchmark server config and logic to llama.cpp-only.
15) Remove or update Ollama-specific tests; add llama.cpp coverage for list/unload/model mgmt.
16) Validate: run core CLI flows and MCP tool-call flow with llama.cpp; run `rg -n "ollama"` to confirm removal.

UNKNOWN
- JSON mode parity (Ollama `format: json` vs llama.cpp `response_format`):
  - Proposed implementation: Mirror existing Ollama JSON mode behavior, but implement it using llama.cpp OpenAI-compatible `response_format: {"type":"json_object"}` where supported and the same validation/retry logic currently used in Ollama paths.
  - Proposed implementation: Standardize JSON enforcement in a shared utility so single, multimodel, pipeline, and MCP paths all use the same logic.
- MCP tool calls (Ollama-specific payloads):
  - Proposed implementation: Use OpenAI-style tool calls for llama.cpp `/v1/chat/completions` and map MCP tools to `tools` + `tool_choice` fields. Decode `tool_calls` responses and forward to MCP server, then append tool results as `role=tool` messages.
  - Proposed implementation: If llama.cpp server lacks tool-call support, add a client-side fallback that detects tool-like intents and routes them to MCP (last resort), or fail with a clear message.
- Model parameters (Ollama Modelfile params):
  - Proposed implementation: Query llama.cpp router-mode `GET /models` with `?model=...` (or equivalent) to fetch model-specific parameters/metadata and display them for `list modelparameters`.
  - Proposed implementation: Use config as a fallback if the server does not provide parameter metadata.
- Pull/delete/sync model management:
  - Proposed implementation: Router-mode only. Use `GET /models` to list; use `POST /models/load` and `POST /models/unload` to approximate pull/delete/sync by controlling loaded set.
  - Proposed implementation: If router mode is not enabled, fail fast with a clear error that these commands require router mode.
- Loaded models (`/api/ps`) visibility:
  - Proposed implementation: For router mode, use `GET /models` state (`loaded/loading/unloaded`) as the source of truth.
  - Proposed implementation: For non-router servers, fail with a clear error stating router mode is required.

Questions
- Do you want MCP tool-calling to hard-require llama.cpp tool call support, or should we implement a client-side fallback?

Llama.cpp API Reference

### 1. Core Inference Endpoints (Native)

These are the high-performance, native endpoints specific to the `llama.cpp` architecture.

| Endpoint | Description | curl Example |
| --- | --- | --- |
| **`POST /completion`** | Primary endpoint for raw text completion. Supports multimodal inputs and streaming. | `curl http://localhost:8080/completion -H "Content-Type: application/json" -d '{"prompt": "The capital of France is", "n_predict": 10}'` |
| **`POST /embedding`** | Generates vector embeddings for a given text. Requires the `--embedding` flag. | `curl http://localhost:8080/embedding -H "Content-Type: application/json" -d '{"content": "Llama is an animal."}'` |
| **`POST /tokenize`** | Converts raw text into a list of model-specific token IDs. | `curl http://localhost:8080/tokenize -H "Content-Type: application/json" -d '{"content": "Hello world"}'` |
| **`POST /detokenize`** | Converts a list of token IDs back into a human-readable string. | `curl http://localhost:8080/detokenize -H "Content-Type: application/json" -d '{"tokens": [15043, 3186]}'` |
| **`POST /rerank`** | Re-orders a list of documents based on relevance to a specific query. | `curl http://localhost:8080/rerank -H "Content-Type: application/json" -d '{"query": "Is it raining?", "documents": ["Sky is blue", "It is wet outside"]}'` |

---

### 2. OpenAI and Anthropic Compatible Endpoints

Used to allow existing applications built for proprietary APIs to work with `llama.cpp` out of the box.

| Endpoint | Description | curl Example |
| --- | --- | --- |
| **`POST /v1/chat/completions`** | Drop-in replacement for OpenAI's chat API. Automatically applies model chat templates. | `curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{"messages": [{"role": "user", "content": "Hi!"}]}'` |
| **`POST /v1/embeddings`** | OpenAI-compatible embedding route. | `curl http://localhost:8080/v1/embeddings -H "Content-Type: application/json" -d '{"input": "Sample text"}'` |
| **`POST /v1/messages`** | Anthropic-compatible (Claude API) route for message-based chat. | `curl http://localhost:8080/v1/messages -H "Content-Type: application/json" -d '{"messages": [{"role": "user", "content": "Explain quantum physics."}]}'` |

---

### 3. Model Management (Router Mode)

These endpoints are active when running `llama-server` in Router Mode (e.g., using `--models-dir`).

| Endpoint | Description | curl Example |
| --- | --- | --- |
| **`GET /models`** | Returns a list of all discovered models and their current state (`loaded`, `unloaded`, `loading`). | `curl http://localhost:8080/models` |
| **`POST /models/load`** | Manually triggers the loading of a specific GGUF model into memory/VRAM. | `curl -X POST http://localhost:8080/models/load -H "Content-Type: application/json" -d '{"model": "llama-3-8b.gguf"}'` |
| **`POST /models/unload`** | Unloads a specific model to free up system resources. | `curl -X POST http://localhost:8080/models/unload -H "Content-Type: application/json" -d '{"model": "llama-3-8b.gguf"}'` |

---

### 4. Monitoring and Utility

Helpful for debugging, load balancing, and health checks.

| Endpoint | Description | curl Example |
| --- | --- | --- |
| **`GET /health`** | Checks server readiness. Returns `503` if the model is still loading, and `200` when ready. | `curl http://localhost:8080/health` |
| **`GET /slots`** | Lists the state of all processing slots (active vs. idle) for parallel decoding. | `curl http://localhost:8080/slots` |
| **`GET /metrics`** | Returns Prometheus-compatible metrics for GPU usage, throughput, and error counts. | `curl http://localhost:8080/metrics` |
| **`POST /props`** | Get or set server properties (like model aliases) at runtime (requires `--props` flag). | `curl http://localhost:8080/props` |

Deliverables
- Codebase references to Ollama removed.
- Config examples and README updated to llama.cpp-only.
- Provider factory and CLI updated to use llama.cpp by default.
- Tests updated/removed accordingly.
- Benchmarks server updated to llama.cpp-only.

Open Questions
1) Should MCP/tool-calling remain, and if so, what is the llama.cpp equivalent route?
2) Are any Ollama-only commands still desired as no-ops for compatibility, or should they be removed?
3) Do we keep the alias `"llamacpp"` in config type parsing?
