{
  "type": "llama.cpp",
  "name": "Falcon3-3B-Instruct-q5_k_m",
  "endpoint": "http://192.168.0.92:9992",
  "gpu": "pentium-n3710-1-60ghz",
  "metadata": {
    "bos_token": "\u003c|endoftext|\u003e",
    "build_info": "b7634-f1768d8f0",
    "chat_template": "{%- if tools %}\n{{- '\u003c|system|\u003e\\n' }}\n{%- if messages[0]['role'] == 'system' %}\n{{- messages[0]['content'] }}\n{%- set remaining_messages = messages[1:] %}\n{%- else %}\n{%- set remaining_messages = messages %}\n{%- endif %}\n{{- 'You are a Falcon assistant skilled in function calling. You are helpful, respectful, and concise.\\n\\n# Tools\\n\\nYou have access to the following functions. You MUST use them to answer questions when needed. For each function call, you MUST return a JSON object inside \u003ctool_call\u003e\u003c/tool_call\u003e tags.\\n\\n\u003ctools\u003e' + tools|tojson(indent=2) + '\u003c/tools\u003e\\n\\n# Output Format\\n\\nYour response MUST follow this format when making function calls:\\n\u003ctool_call\u003e\\n[\\n  {\"name\": \"function_name\", \"arguments\": {\"arg1\": \"value1\", \"arg2\": \"value2\"}},\\n  {\"name\": \"another_function\", \"arguments\": {\"arg\": \"value\"}}\\n]\\n\u003c/tool_call\u003e\\nIf no function calls are needed, respond normally without the tool_call tags.\\n' }}\n{%- for message in remaining_messages %}\n{%- if message['role'] == 'user' %}\n{{- '\u003c|user|\u003e\\n' + message['content'] + '\\n' }}\n{%- elif message['role'] == 'assistant' %}\n{%- if message.content %}\n{{- '\u003c|assistant|\u003e\\n' + message['content'] }}\n{%- endif %}\n{%- if message.tool_calls %}\n{{- '\\n\u003ctool_call\u003e\\n' }}\n{{- message.tool_calls|tojson(indent=2) }}\n{{- '\\n\u003c/tool_call\u003e' }}\n{%- endif %}\n{{- eos_token + '\\n' }}\n{%- elif message['role'] == 'tool' %}\n{{- '\u003c|assistant|\u003e\\n\u003ctool_response\u003e\\n' + message['content'] + '\\n\u003c/tool_response\u003e\\n' }}\n{%- endif %}\n{%- endfor %}\n{{- '\u003c|assistant|\u003e\\n' if add_generation_prompt }}\n{%- else %}\n{%- for message in messages %}\n{%- if message['role'] == 'system' %}\n{{- '\u003c|system|\u003e\\n' + message['content'] + '\\n' }}\n{%- elif message['role'] == 'user' %}\n{{- '\u003c|user|\u003e\\n' + message['content'] + '\\n' }}\n{%- elif message['role'] == 'assistant' %}\n{%- if not loop.last %}\n{{- '\u003c|assistant|\u003e\\n' + message['content'] + eos_token + '\\n' }}\n{%- else %}\n{{- '\u003c|assistant|\u003e\\n' + message['content'] + eos_token }}\n{%- endif %}\n{%- endif %}\n{%- if loop.last and add_generation_prompt %}\n{{- '\u003c|assistant|\u003e\\n' }}\n{%- endif %}\n{%- endfor %}\n{%- endif %}",
    "default_generation_settings": {
      "n_ctx": 32768,
      "params": {
        "backend_sampling": false,
        "chat_format": "Content-only",
        "dry_allowed_length": 2,
        "dry_base": 1.75,
        "dry_multiplier": 0,
        "dry_penalty_last_n": -1,
        "dynatemp_exponent": 1,
        "dynatemp_range": 0,
        "frequency_penalty": 0,
        "ignore_eos": false,
        "lora": [],
        "max_tokens": -1,
        "min_keep": 0,
        "min_p": 0.05000000074505806,
        "mirostat": 0,
        "mirostat_eta": 0.10000000149011612,
        "mirostat_tau": 5,
        "n_discard": 0,
        "n_keep": 0,
        "n_predict": -1,
        "n_probs": 0,
        "post_sampling_probs": false,
        "presence_penalty": 0,
        "reasoning_format": "none",
        "reasoning_in_content": false,
        "repeat_last_n": 64,
        "repeat_penalty": 1,
        "samplers": [
          "penalties",
          "dry",
          "top_n_sigma",
          "top_k",
          "typ_p",
          "top_p",
          "min_p",
          "xtc",
          "temperature"
        ],
        "seed": 4294967295,
        "speculative.n_max": 16,
        "speculative.n_min": 0,
        "speculative.p_min": 0.75,
        "stream": true,
        "temperature": 0.800000011920929,
        "thinking_forced_open": false,
        "timings_per_token": false,
        "top_k": 40,
        "top_n_sigma": -1,
        "top_p": 0.949999988079071,
        "typical_p": 1,
        "xtc_probability": 0,
        "xtc_threshold": 0.10000000149011612
      }
    },
    "endpoint_metrics": true,
    "endpoint_props": false,
    "endpoint_slots": true,
    "eos_token": "\u003c|endoftext|\u003e",
    "is_sleeping": false,
    "modalities": {
      "audio": false,
      "vision": false
    },
    "model_alias": "Falcon3-3B-Instruct-q5_k_m",
    "model_path": "/home/matt/projects/gollama/models/Falcon3-3B-Instruct-q5_k_m.gguf",
    "total_slots": 4,
    "webui": true,
    "webui_settings": {}
  }
}